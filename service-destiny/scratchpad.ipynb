{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from agent import Agent\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# # Sample user input\n",
    "# user_data = {\n",
    "#     \"name\": \"Ravi\",\n",
    "#     \"hobbies\": [\"movies\", \"reading\", \"travel\"],\n",
    "#     \"summary\": \"I'm an easygoing person who loves exploring new places and watching good films.\"\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/w7jht0dd48scmmszn8d0zz300000gn/T/ipykernel_52373/3962976665.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
      "/var/folders/3w/w7jht0dd48scmmszn8d0zz300000gn/T/ipykernel_52373/3962976665.py:23: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Assistant: Hey Roshan! I noticed you enjoy yoga. What do you love most about your practice?\n",
      "ðŸ¤– Assistant: That sounds great! Do you have a favorite type of yoga or a specific pose you enjoy the most?\n",
      "ðŸ¤– Assistant: It seems like you might be busy. No worries! If you want to share later, I'm here. Thank you for chatting, this information helps us improve the recommendation algorithm to find better matches for you.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "import yaml\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "def load_prompts(filepath: str) -> dict:\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "name = 'Roshan'\n",
    "hobbies = 'yoga, gaming'\n",
    "    \n",
    "prompts = load_prompts('prompts.yaml')\n",
    "system_prompt = prompts['system_prompt'].format(name = name, hobbies = hobbies)\n",
    "\n",
    "# Start with system message if no history\n",
    "messages = []\n",
    "messages.append(SystemMessage(content=system_prompt))\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Now call the model with the message list\n",
    "    response = llm(messages)\n",
    "\n",
    "    # Append and print the response\n",
    "    messages.append(AIMessage(content=response.content))\n",
    "\n",
    "    if 'thank you for chatting' in response.content.lower():\n",
    "        break\n",
    "\n",
    "    print(f\"ðŸ¤– Assistant: {response.content}\")\n",
    "    user_input = input(\"ðŸ‘¤ You: \")\n",
    "    if user_input.lower() in ['bye', 'exit', 'quit']:\n",
    "        goodbye = \"Thank you for chatting, this information helps us find better matches for you.\"\n",
    "        print(f\"ðŸ¤– Assistant: {goodbye}\")\n",
    "        break\n",
    "\n",
    "    messages.append(HumanMessage(content=user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a friendly assistant chatting with users about their hobbies like painting and gaming. Ask them follow-up questions based on their replies.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi Roshan! You said you like painting and gaming. What kind of painting do you enjoy?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='i like vangogh', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Vincent van Gogh is such an inspiring artist! Do you have a favorite painting of his? And have you tried to replicate any of his styles in your own work?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='yeah i have tried impressionalism in my work', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That sounds fantastic! Impressionism has such a beautiful way of capturing light and movement. What subjects do you usually choose to paint in that style?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='nature, trees, butterflies', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nature is such a wonderful subject for impressionism! Do you have a favorite spot or location where you go to find inspiration for your paintings?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3w/w7jht0dd48scmmszn8d0zz300000gn/T/ipykernel_49994/3179901216.py:16: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "# Step 1: Define output schema\n",
    "class ConversationResponse(BaseModel):\n",
    "    messages: List[Dict[str, str]] = Field(\n",
    "        ..., description=\"Alternating assistant/user messages\"\n",
    "    )\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ConversationResponse)\n",
    "\n",
    "# Step 2: Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Step 3: Create agent\n",
    "conversation_agent = Agent(\n",
    "    role=\"Conversational Match Assistant\",\n",
    "    goal=\"Understand user preferences based on hobbies\",\n",
    "    backstory=\"Friendly AI helping users share more about themselves to improve matching.\",\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b81236403940de9d01964fcedd5ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4e7512ad1349389ec3a60bfde14963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Type your message...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "history = []\n",
    "\n",
    "# Create widgets\n",
    "input_box = widgets.Text(placeholder='Type your message...')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def handle_input(change):\n",
    "    user_input = change.new\n",
    "    if not user_input.strip():\n",
    "        return\n",
    "    input_box.value = ''  # clear input\n",
    "    with output_area:\n",
    "        print(f\"ðŸ‘¤ You: {user_input}\")\n",
    "    \n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    response = conversation_agent.llm.predict_messages(history)\n",
    "    reply = response.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    \n",
    "    with output_area:\n",
    "        print(f\"ðŸ¤– Assistant: {reply}\\n\")\n",
    "\n",
    "# Bind input handler\n",
    "input_box.observe(handle_input, names='value')\n",
    "\n",
    "# Display chat\n",
    "display(output_area, input_box)\n",
    "#print(\"Type your message and press Enter. Type 'exit' to quit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_obj = Agent(user_data)\n",
    "# agent_obj.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "destiny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
